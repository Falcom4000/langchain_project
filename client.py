import asyncio
import os
import json
import sys
from typing import Optional, Dict, Any, List
from contextlib import AsyncExitStack
from dotenv import load_dotenv
from mcp import ClientSession, StdioServerParameters
from mcp.client.stdio import stdio_client
from langchain_community.chat_models import ChatOpenAI
from langchain.output_parsers import PydanticOutputParser
from langchain_core.prompts import PromptTemplate
from langchain_community.document_loaders import TextLoader, CSVLoader
from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import OpenAIEmbeddings
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.schema import Document
from pydantic import BaseModel, Field
import pandas as pd

# åŠ è½½ .env æ–‡ä»¶ï¼Œç¡®ä¿ API Key å—åˆ°ä¿æŠ¤
load_dotenv()

class QuerySchema(BaseModel):
    """
    å®šä¹‰æŸ¥è¯¢çš„ Pydantic æ¨¡å‹
    """
    mcp_tool: str = Field(..., description="éœ€è¦è°ƒç”¨çš„mcp tool")
    mcp_input: Dict[str, Any] = Field(default_factory=dict, description="mcp toolçš„è¾“å…¥å‚æ•°ï¼Œå¿…é¡»æ˜¯å­—å…¸æ ¼å¼")

    model_config = {
        "json_schema_extra": {
            "examples": [
                {
                    "mcp_tool": "csv_info",
                    "mcp_input": {}
                },
                {
                    "mcp_tool": "csv_plot", 
                    "mcp_input": {"col_name": "age"}
                }
            ]
        }
    }

class RAGManager:
    """RAGçŸ¥è¯†åº“ç®¡ç†å™¨"""
    
    def __init__(self, openai_api_key: str, base_url: Optional[str] = None):
        self.embeddings = OpenAIEmbeddings(
            api_key=openai_api_key,
            base_url=base_url
        )
        self.vectorstore: Optional[FAISS] = None
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1000,
            chunk_overlap=200,
            length_function=len
        )
    
    async def load_documents(self, doc_paths: List[str]) -> List[Document]:
        """åŠ è½½å¤šç§ç±»å‹çš„æ–‡æ¡£"""
        documents = []
        
        for path in doc_paths:
            if not os.path.exists(path):
                print(f"âš ï¸ æ–‡ä»¶ä¸å­˜åœ¨: {path}")
                continue
                
            file_ext = os.path.splitext(path)[1].lower()
            
            try:
                if file_ext == '.csv':
                    # åŠ è½½CSVæ–‡ä»¶
                    df = pd.read_csv(path)
                    # åˆ›å»ºCSVæ¦‚è§ˆæ–‡æ¡£
                    content = f"CSVæ–‡ä»¶æ¦‚è§ˆ - {os.path.basename(path)}:\n"
                    content += f"å½¢çŠ¶: {df.shape}\n"
                    content += f"åˆ—å: {', '.join(df.columns.tolist())}\n"
                    content += f"æ•°æ®ç±»å‹:\n{df.dtypes.to_string()}\n"
                    content += f"å‰5è¡Œæ•°æ®:\n{df.head().to_string()}\n"
                    content += f"ç»Ÿè®¡ä¿¡æ¯:\n{df.describe().to_string()}"
                    
                    doc = Document(
                        page_content=content,
                        metadata={"source": path, "type": "csv"}
                    )
                    documents.append(doc)
                    
                elif file_ext in ['.txt', '.md']:
                    # åŠ è½½æ–‡æœ¬æ–‡ä»¶
                    loader = TextLoader(path)
                    docs = loader.load()
                    documents.extend(docs)
                    
                print(f"âœ… å·²åŠ è½½æ–‡æ¡£: {path}")
                
            except Exception as e:
                print(f"âŒ åŠ è½½æ–‡æ¡£å¤±è´¥ {path}: {e}")
        
        return documents
    
    async def build_vectorstore(self, documents: List[Document]):
        """æ„å»ºå‘é‡æ•°æ®åº“"""
        if not documents:
            print("âš ï¸ æ²¡æœ‰æ–‡æ¡£ç”¨äºæ„å»ºå‘é‡æ•°æ®åº“")
            return
            
        # åˆ†å‰²æ–‡æ¡£
        texts = self.text_splitter.split_documents(documents)
        print(f"ğŸ“„ æ–‡æ¡£åˆ†å‰²å®Œæˆï¼Œå…± {len(texts)} ä¸ªç‰‡æ®µ")
        
        # æ„å»ºå‘é‡æ•°æ®åº“
        self.vectorstore = await FAISS.afrom_documents(texts, self.embeddings)
        print("âœ… å‘é‡æ•°æ®åº“æ„å»ºå®Œæˆ")
    
    async def retrieve_context(self, query: str, k: int = 3) -> str:
        """æ£€ç´¢ç›¸å…³ä¸Šä¸‹æ–‡"""
        if not self.vectorstore:
            return ""
            
        try:
            docs = await self.vectorstore.asimilarity_search(query, k=k)
            context = "\n\n".join([doc.page_content for doc in docs])
            return context
        except Exception as e:
            print(f"âŒ æ£€ç´¢ä¸Šä¸‹æ–‡å¤±è´¥: {e}")
            return ""

class MCPClient:
    def __init__(self):
        """åˆå§‹åŒ– MCP å®¢æˆ·ç«¯"""
        self.exit_stack = AsyncExitStack()
        # ä»ç¯å¢ƒå˜é‡è¯»å–é…ç½®
        self.openai_api_key = os.getenv("OPENAI_API_KEY")
        self.base_url = os.getenv("BASE_URL")
        self.model = os.getenv("MODEL")

        if not self.openai_api_key:
            raise ValueError("âŒ æœªæ‰¾åˆ° OpenAI API Keyï¼Œè¯·åœ¨ .env æ–‡ä»¶ä¸­è®¾ç½® OPENAI_API_KEY")

        # åˆå§‹åŒ– OpenAI å®¢æˆ·ç«¯
        self.client = ChatOpenAI(
            api_key=self.openai_api_key, 
            base_url=self.base_url, 
            model=self.model, 
            temperature=0.7
        )
        
        # åˆå§‹åŒ– RAG ç®¡ç†å™¨
        self.rag_manager = RAGManager(self.openai_api_key, self.base_url)
        
        self.session: Optional[ClientSession] = None
        self.parser = PydanticOutputParser(pydantic_object=QuerySchema)
        self.prompt_template = PromptTemplate(
            template="æ ¹æ®ç”¨æˆ·çš„è¾“å…¥é€‰æ‹©åˆé€‚çš„MCP Tool.\n{format_instructions}\n{input}\n",
            input_variables=["input"],
            partial_variables={"format_instructions": self.parser.get_format_instructions()}
        )
        self.chat = self.prompt_template | self.client | self.parser

    async def initialize_rag(self, knowledge_base_paths: List[str]):
        """åˆå§‹åŒ–RAGçŸ¥è¯†åº“"""
        print("ğŸ”„ æ­£åœ¨åˆå§‹åŒ–RAGçŸ¥è¯†åº“...")
        documents = await self.rag_manager.load_documents(knowledge_base_paths)
        await self.rag_manager.build_vectorstore(documents)
        print("âœ… RAGçŸ¥è¯†åº“åˆå§‹åŒ–å®Œæˆ")

    async def connect_to_server(self, server_script_path: str):
        """è¿æ¥åˆ° MCP æœåŠ¡å™¨å¹¶åˆ—å‡ºå¯ç”¨å·¥å…·"""
        # æ ¹æ®è„šæœ¬ç±»å‹é€‰æ‹©æ‰§è¡Œå‘½ä»¤
        server_params = StdioServerParameters(
            command="python",
            args=[server_script_path],
            env=None
        )

        # å¯åŠ¨ MCP æœåŠ¡å™¨å¹¶å»ºç«‹é€šä¿¡
        stdio_transport = await self.exit_stack.enter_async_context(
            stdio_client(server_params)
        )
        self.stdio, self.write = stdio_transport
        self.session = await self.exit_stack.enter_async_context(
            ClientSession(self.stdio, self.write)
        )
        await self.session.initialize()

        # åˆ—å‡º MCP æœåŠ¡å™¨ä¸Šçš„å·¥å…·
        response = await self.session.list_tools()
        tools = response.tools
        print("\nå·²è¿æ¥åˆ°æœåŠ¡å™¨ï¼Œæ”¯æŒä»¥ä¸‹å·¥å…·:", [tool.name for tool in tools])
        
        tools_description = "\n".join([f"- {tool.name}: {tool.description}" for tool in tools])
        
        # æ›´æ–°prompt templateä»¥æ”¯æŒRAGä¸Šä¸‹æ–‡
        self.prompt_template = PromptTemplate(
            template="""æ ¹æ®ç”¨æˆ·çš„è¾“å…¥å’Œç›¸å…³ä¸Šä¸‹æ–‡ä¿¡æ¯é€‰æ‹©åˆé€‚çš„MCP Toolã€‚

å¯ç”¨çš„å·¥å…·:
{tools}

ç›¸å…³ä¸Šä¸‹æ–‡ä¿¡æ¯:
{context}

è¯·æ ¹æ®ç”¨æˆ·æŸ¥è¯¢å’Œä¸Šä¸‹æ–‡ä¿¡æ¯é€‰æ‹©æœ€åˆé€‚çš„å·¥å…·ã€‚
- å¦‚æœå·¥å…·ä¸éœ€è¦å‚æ•°ï¼Œmcp_input åº”è¯¥æ˜¯ç©ºå­—å…¸ {{}}
- å¦‚æœå·¥å…·éœ€è¦å‚æ•°ï¼Œmcp_input åº”è¯¥æ˜¯åŒ…å«å‚æ•°çš„å­—å…¸ï¼Œä¾‹å¦‚ {{"col_name": "age"}}

{format_instructions}

ç”¨æˆ·è¾“å…¥: {input}
""",
            input_variables=["input", "context"],
            partial_variables={
                "format_instructions": self.parser.get_format_instructions(),
                "tools": tools_description
            }
        )
        
        self.chat = self.prompt_template | self.client | self.parser

    async def process_query(self, query: str):
        """
        ä½¿ç”¨RAGå¢å¼ºçš„æŸ¥è¯¢å¤„ç†
        """
        if not self.session:
            raise RuntimeError("âŒ è¿˜æœªè¿æ¥åˆ° MCP æœåŠ¡å™¨ï¼Œè¯·å…ˆè°ƒç”¨ connect_to_server()")

        # RAGæ£€ç´¢ç›¸å…³ä¸Šä¸‹æ–‡
        context = await self.rag_manager.retrieve_context(query)
        if context:
            print(f"ğŸ” æ£€ç´¢åˆ°ç›¸å…³ä¸Šä¸‹æ–‡ ({len(context)} å­—ç¬¦)")
        
        # ä½¿ç”¨å¤§æ¨¡å‹å¤„ç†æŸ¥è¯¢ï¼ˆåŒ…å«ä¸Šä¸‹æ–‡ï¼‰
        response = await self.chat.ainvoke({"input": query, "context": context})
        print(f"\nğŸ¤– LLM å“åº”: {response}")

        # è°ƒç”¨ MCP å·¥å…·
        tool_name = response.mcp_tool
        tool_input = response.mcp_input

        if not tool_name:
            raise ValueError("âŒ æœªæŒ‡å®šè¦è°ƒç”¨çš„ MCP Tool")

        result = await self.session.call_tool(tool_name, arguments=tool_input)
        
        return result.content[0].text

    async def chat_loop(self):
        """è¿è¡Œäº¤äº’å¼èŠå¤©å¾ªç¯"""
        print("\nğŸ¤– Persimmon MCP å®¢æˆ·ç«¯å·²å¯åŠ¨ï¼è¾“å…¥ 'quit' é€€å‡º")
        while True:
            try:
                query = input("\nä½ : ").strip()
                if query.lower() == 'quit':
                    break
                response = await self.process_query(query)  # å‘é€ç”¨æˆ·è¾“å…¥åˆ° OpenAI API
                print(f"\nğŸ¤– LLM: {response}")
            except Exception as e:
                print(f"\nâš ï¸ å‘ç”Ÿé”™è¯¯: {str(e)}")

    async def cleanup(self):
        """æ¸…ç†èµ„æº"""
        await self.exit_stack.aclose()


async def main():
    """ä¸»å‡½æ•°"""
    if len(sys.argv) < 2:
        print("Usage: python client.py <path_to_server_script> [knowledge_base_paths...]")
        print("Example: python client.py server.py data/titanic_cleaned.csv docs/readme.txt")
        sys.exit(1)

    client = MCPClient()
    try:
        # åˆå§‹åŒ–RAGçŸ¥è¯†åº“ï¼ˆå¦‚æœæä¾›äº†çŸ¥è¯†åº“è·¯å¾„ï¼‰
        if len(sys.argv) > 2:
            knowledge_paths = sys.argv[2:]
            await client.initialize_rag(knowledge_paths)
        
        await client.connect_to_server(sys.argv[1])
        await client.chat_loop()
    finally:
        await client.cleanup()


if __name__ == "__main__":
    asyncio.run(main())
